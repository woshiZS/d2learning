### 一些需要注意的点

* 关于重新分配内存的问题

```python
id(X)
X = X + Y 
id(X)
```

这种前后的地址是不一样的，如果要防止内存进行重新分配，需要这么写

```python
X[:] = X + Y
```

* 另外就是深浅拷贝的问题

```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A	
B += 3 # 此时A的值也会发生改变
C = A.clone()
C += 3 # A的值不会发生改变
```

* sum方法的axis可以理解为沿着哪个轴求和，张量就在哪个维度上进行坍塌。
* 导数拓展到向量（一般称作梯度）

首先是常量对向量求导（其实就是偏导），也会的得到一个向量（先不管这里是行向量还是列向量），这个向量所指的方向就是函数变化最快的方向，即梯度。常量对列向量求导编程行向量，列向量对常量求导还是列向量，列向量对列向量求导就变成了矩阵。

* 自动求导

其实倒数这一块主要还是要理解结果的维度方向。符号求导和数值求导。以及代码计算导数其实是有一个计算图的概念在里面的，以符合求导为例，每层展开替换都会被构建成一个有向无环图，细节的原理不做深究，知道是这么一个流程·即可。

大致流程就是构建一个require_grads的tensor，对这个tensor所作的一系列操作最后的一个值求backward, 就能得到我们对原来那个tensor的导数。